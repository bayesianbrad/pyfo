{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1  1\n",
      " 2  2  2\n",
      " 3  3  3\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "True grad for : \n",
      "-991.4100   -1.0740   -0.8943\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "True grad for : \n",
      " -4.5386  -4.5931 -10.1301\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "\n",
      "-991.4100   -1.0740   -0.8943\n",
      "   2.0000    2.0000    2.0000\n",
      "  -4.5386   -4.5931  -10.1301\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "<class 'torch.FloatTensor'> torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import scipy.stats as ss\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "d = 3\n",
    "a  = Variable(torch.ones(1,d), requires_grad = True)\n",
    "b = Variable(2*torch.ones(1,d), requires_grad  = True)\n",
    "c = Variable(3*torch.ones(1,d), requires_grad  = True)\n",
    "x = Variable(torch.FloatTensor(1,d).zero_())\n",
    "params_dic = {}\n",
    "params  = [a,b,c]\n",
    "for i in range(d):\n",
    "    params_dic[\"var{0}\".format(i)] = params[i]\n",
    "#print(params_dic)\n",
    "# as the first element needs to be assigned to x\n",
    "x = params[0]\n",
    "# then concatenate the rest of params\n",
    "for i in range(len(params)):\n",
    "    if i==0:\n",
    "        x  = params[i]\n",
    "    else:\n",
    "        x = torch.cat((x,params[i]), dim = 0)\n",
    "print(x)\n",
    "# mvn = ss.multivariate_normal.logpdf(a.data.numpy(), mean  = np.zeros(d), cov =  np.eye(d))\n",
    "# mvn.backward()\n",
    "# unpack x\n",
    "# for i in range(len(params)):\n",
    "#     y += torch.log(params[i].mm(params[i].t()))\n",
    "def log_normal(value):\n",
    "    value = value.unsqueeze(-1).t()\n",
    "    mean = Variable(torch.rand(3).unsqueeze(-1).t())\n",
    "    std  = Variable(torch.rand(3).unsqueeze(-1).t())\n",
    "    true_grad = -(value.data  - mean.data)/std.data**2\n",
    "    print('True grad for :', true_grad)\n",
    "    return (-0.5 *  torch.pow(value - mean, 2) / std**2) -  torch.log(std)\n",
    "y1 = log_normal(x[0,:])\n",
    "y2 = 2*x[1,:].unsqueeze(-1).t()\n",
    "y3 = log_normal(x[2,:])\n",
    "# print(y1,y2,y3)\n",
    "y = y1+y2+y3\n",
    "# print(y)\n",
    "gradients = torch.autograd.grad(outputs=y,inputs=x, grad_outputs = torch.ones(3,3), retain_graph = True)\n",
    "print(gradients[0].data * 1/x.data.size()[1])\n",
    "print(type(gradients[0].data), gradients[0].data.size())\n",
    "# y.backward()\n",
    "# print(a.grad)\n",
    "# print(b.grad)\n",
    "# print(c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.4865  0.2012  0.0591\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Distributions.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auto_grad_test(a,b,c,params):  \n",
    "    y = 0\n",
    "    for i in range(len(params)):\n",
    "        y += torch.log(params[i]) \n",
    "    y = torch.log(x)    \n",
    "    gradients = torch.autograd.grad(outputs=[y,inputs=[a], retain_graph= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-ddf0172ac555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauto_grad_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-24e56f303e57>\u001b[0m in \u001b[0;36mauto_grad_test\u001b[0;34m(a, b, c, params)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mgrad_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, user_create_graph)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 new_grads.append(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "auto_grad_test(a,b,c,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
